{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOWRUv+3tkDk9KVb1iEvWTy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jagadish9084/ml-practice/blob/main/data_preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "aSVIPGo3xjel"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DataPreprocessor:\n",
        "  def __init__(self, data, target_variable, should_handle_missing_value =True):\n",
        "    self.data = data\n",
        "    self.target_variable = target_variable\n",
        "    self.should_handle_missing_value = should_handle_missing_value\n",
        "  # Find columns with missing values\n",
        "  def find_columns_with_missing_values(self):\n",
        "    return [col for col, count in self.data.isna().sum().items() if count > 0]\n",
        "\n",
        "  # Handle missing values for both numerical and categorical columns\n",
        "  def hanlde_missing_values(self):\n",
        "    columns_to_be_handled = self.find_columns_with_missing_values()\n",
        "    print(f'Columns with missing values before cleaning:{columns_to_be_handled}')\n",
        "    for col in columns_to_be_handled:\n",
        "      if self.data[col].dtype == 'int' or self.data[col].dtype=='float':\n",
        "        self.data[col].fillna(self.data[col].mean(), inplace =True)\n",
        "      else:\n",
        "        self.data[col].fillna(self.data[col].mode()[0], inplace =True)\n",
        "    columns_to_be_handled = self.find_columns_with_missing_values()\n",
        "    print(f'Columns with missing values after cleaning:{columns_to_be_handled}')\n",
        "\n",
        "  # Peroform OneHutEncoding for categorical columns\n",
        "  def encode_categories(self, cat_col):\n",
        "    print(f'Categorical columns to be encoded: {cat_col}')\n",
        "    encoder = OneHotEncoder(drop='first', sparse_output=False, dtype ='int')\n",
        "    out = encoder.fit_transform(self.data[cat_col])\n",
        "    encoded_dataframe = pd.DataFrame(out, columns=encoder.get_feature_names_out())\n",
        "    encoded_dataframe.columns = encoded_dataframe.columns.str.replace('\\w+_','', regex=True)\n",
        "    return encoded_dataframe\n",
        "\n",
        "  def preprocess(self):\n",
        "\n",
        "    if self.should_handle_missing_value:\n",
        "      self.hanlde_missing_values()\n",
        "\n",
        "    cat_col =  [col for col in self.data.columns if self.data[col].dtype !='float' and self.data[col].dtype !='int']\n",
        "    encoded_data = self.encode_categories(cat_col)\n",
        "\n",
        "    # Drop encoded columns from original data set\n",
        "    self.data.drop(cat_col, axis=1, inplace=True)\n",
        "\n",
        "    #Concat original data set with encoded columns\n",
        "    self.data = pd.concat([self.data, encoded_data], axis=1)\n",
        "\n",
        "    #Seperate dependent and independent variable\n",
        "    y, x = self.data[self.target_variable], self.data.drop([self.target_variable], axis=1)\n",
        "\n",
        "    # Sweep the original data from memory\n",
        "    del self.data\n",
        "    # Split traing and testing data\n",
        "    self.x_train, self.x_test, self.y_train, self.y_test = train_test_split(x, y, test_size = 0.30, random_state=42)\n",
        "\n",
        "    # Standardize the data\n",
        "    scalar  = StandardScaler()\n",
        "    self.x_train = pd.DataFrame(scalar.fit_transform(self.x_train), columns= scalar.get_feature_names_out())\n",
        "    self.x_test = pd.DataFrame(scalar.transform(self.x_test), columns= scalar.get_feature_names_out())\n"
      ],
      "metadata": {
        "id": "pqelP5KvLPcT"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_set = sns.load_dataset('flights')\n",
        "target_variable = 'passengers'\n",
        "preprocessor = DataPreprocessor(data_set, target_variable)\n",
        "preprocessor.preprocess()\n",
        "preprocessor.y_test"
      ],
      "metadata": {
        "id": "h94AMzOqGyM4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}